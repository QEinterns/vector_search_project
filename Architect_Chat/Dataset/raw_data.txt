0 50 k100 k150 k200 k250 k300 k350 k400 k450 k Workload A Workload F UpdateOnlyThroughput (ops/sec)Magma (item counts disabled) Magma (item counts enabled) Couchstore 0.0 50.0 k100.0 k150.0 k200.0 k250.0 k300.0 k350.0 k  0  5  10  15  20  25  30  35Throughput (ops/s) Number of threadsMagma RocksDB Couchstore Magma async I/O KV-Engine Storage Requirements Dave Rigby 2018-01-18 Introduction This document lists the requirements KV-Engine has on the underlying storage engine. To explain why these are necessary , it also describes some of the high-level architecture and design of KV-Engine where relevant to storage. The aim of the document is to (a) document the features of the current storage engine (couchstore) which KV-Engine relies on, and (b) to provide a set of requirements for potential alternative storage engines. KV-Engine Architecture ' Everything should be made as simple as possible, but no simpler . ” KV-Engine is responsible for storing Documents ( Values ) which may be looked up using their Key . It aims to do this in a fast and scalable manner, such that ~millions of requests/s can be handled by a single node. This is achieved by a number of fundamental principles: ● Simple - Clients can only access documents by key, and only access one document at once (no range queries). Only relatively simple operations are supported on a given document. ● Memory-centric - Documents are initially written to RAM; with persistence and replication happening asynchronously . Frequently accessed documents are kept in RAM so avoid having to fetch them from disk. ● Distributed, clustered data store - A bucket is sharded into N vBuckets; each of which is assigned a single node to own the active copy. 0-3 replica copies of each vBucket can be configured, each of which is assigned to a different node. Updates are transmitted asynchronously from active -> replica(s) via TCP (using the DCP protocol). ● Eventually Persistent - Updates are not written to disk in the main request/response path - a request is considered 'successful' as soon as the write has been accepted into RAM. Background flusher (writer) tasks batch mutations and write them to disk asynchronously . Together this allows us to build a data store which is both performant (by minimising disk in the common path) and reliable (by using replication and background persistence to guard against node failure). See KV-Engine Architectural Overview for a more detailed description. KV-Engine Storage Requirements Dave Rigby 2018-01-18 Introduction This document lists the requirements KV-Engine has on the underlying storage engine. To explain why these are necessary , it also describes some of the high-level architecture and design of KV-Engine where relevant to storage. The aim of the document is to (a) document the features of the current storage engine (couchstore) which KV-Engine relies on, and (b) to provide a set of requirements for potential alternative storage engines. KV-Engine Architecture ' Everything should be made as simple as possible, but no simpler . ” KV-Engine is responsible for storing Documents ( Values ) which may be looked up using their Key . It aims to do this in a fast and scalable manner, such that ~millions of requests/s can be handled by a single node. This is achieved by a number of fundamental principles: ● Simple - Clients can only access documents by key, and only access one document at once (no range queries). Only relatively simple operations are supported on a given document. ● Memory-centric - Documents are initially written to RAM; with persistence and replication happening asynchronously . Frequently accessed documents are kept in RAM so avoid having to fetch them from disk. ● Distributed, clustered data store - A bucket is sharded into N vBuckets; each of which is assigned a single node to own the active copy. 0-3 replica copies of each vBucket can be configured, each of which is assigned to a different node. Updates are transmitted asynchronously from active -> replica(s) via TCP (using the DCP protocol). ● Eventually Persistent - Updates are not written to disk in the main request/response path - a request is considered 'successful' as soon as the write has been accepted into RAM. Background flusher (writer) tasks batch mutations and write them to disk asynchronously . Together this allows us to build a data store which is both performant (by minimising disk in the common path) and reliable (by using replication and background persistence to guard against node failure). See KV-Engine Architectural Overview for a more detailed description. 0 50 k100 k150 k200 k250 k300 k350 k Workload B Workload C Workload DThroughput (ops/sec)Magma (item counts disabled) Magma (item counts enabled) Couchstore  0 500 1000 1500 2000 2500 3000 512B 1KB 2KB 4KB 8KB 16KB 32KB 64KBThroughput (MB/s) Value sizeMagma RocksDB Couchstore  0 50 100 150 200 250 300 350 400 450 512B 1KB 2KB 4KB 8KB 16KB 32KB 64KBThroughput (MB/s) Value sizeMagma RocksDB Couchstore  0 2 4 6 8 10 12 14 16 18 20 Load Update Round 1 Update Round 2Write amplificationMagma RocksDB Couchstore Magma: A high density key-value storage engine used in Couchbase Abstract 2 Introduction 3 Background 5 Couchbase Data Service 5 Managed Document Cache 6 Database Change Protocol 6 Copy-On-Write Log-Structured B+Tree 7 Modification operation 7 Read operation 8 Compaction operation 8 Couchstore 8 Challenges with high data density 9 Opportunities for caching reduces 9 Write amplification 9 Compaction 10 Log-Structured Merge Tree 10 Write Operation 11 Compaction Operation 11 Read operation 12 Magma Design 12 Design Goals 13 Lower write amplification 13 Scalable concurrent compaction 13 Leverage sequential I/O 13 Minimize memory usage 13 Design overview 14 Architectural Components 14 Write cache 14 Write ahead log 15 LSM Tree index 15 Log structured object store 15 Index Block cache 15 Separation of index and data organization 16 Wisckey 16 Magma : Change data capture Introduction As detailed in this document, there is a requirement for CB Server to retain and provide all versions of a document within a finite window. Currently, documents with the same key are de-duplicated in memory by KV-Engine, garbage collection by the storage engine on disk, and DCP provides only the latest copy of a document. This document will discuss the changes needed to support storing and retaining all older versions of a document in Magma within a configured window and streaming them to KV-Engine for DCP based backfills. The proposed solution can also be used for DCP based PiTR( PRD + Magma Design doc ) Background Data in a bucket on a single node is divided into multiple shards. Each shard is a Magma instance which further contains vbuckets/kvstores. Each KVStore in Magma consists of a key index, sequence index, and local index. Sequence Index as a log of all operations The key and sequence index together store document data in key and sequence number order respectively. The key index is implemented as a Log Structured Merge Tree which is a write-optimized data structure allowing for higher write throughput and background compaction. The LSMTrees used by the key index have high write amplification(up to 40X). To minimize write amplification, Magma employs value-separation which involves storing the value of a document that exceeds a size threshold in a separate data structure. Since every new update or delete operation generates a monotonically incrementing sequence number. That means the sequence index always grows at the tail providing a log of all incoming mutations which has low write amp and can be used to store the values. It is implemented as a segmented log as shown in the diagram. 0 50 k100 k150 k200 k250 k300 k Load Update Round 1 Update Round 2Throughput (ops/sec)Magma RocksDB Couchstore Log Structured Deletes (LSD) Background The magma storage engine consists of three components. A byKey Index, bySeq Index and Value store. The byKey Index serves the purpose of answering point-lookup based on document key. The bySeq index serves range queries based on document sequence based ranges for a vbucket. We use LSM Tree for implementing the byKey index. If we place document value along with the key in the byKey index, when LSM Tree runs compaction operations to maintain the tree balance for read and space amplification, value gets rewritten many times (Upto 40x for a 4 level tree). To overcome unnecessary write amplification, Magma places values in a separate log structured storage and uses record-offset (physical file based addressing) as value pointer for large objects. Instead of actual values, a value pointer is stored in the keyIndex in the place of value along with the key. This comes with additional work of maintaining a separate storage system tuned for storing large values. When the value storage internal segments (unit of storage) becomes fragmented, we have to clean the segments by rewriting the values to a new segment. Since, the value pointers are physical offsets derived based on segments, position within the blocks of the segment, when valid values are rewritten to a new segment, the original value pointers become invalid. For valid values, we have to update the corresponding value pointer offsets in the byKeyIndex. While cleaning a value store segment, we have to determine whether each value if valid or not by performing a lookup into the byKey index. This involves a IO operation per value. We optimize this cost by avoiding lookup for most of the obsolete values by maintaining a garbage collection validity bitmap (1 bit per value object). The obsolete bits are set during compaction performed by byKey index. When documents are added to a Magma kvstore, an entry <key, value> or <key, valueptr> is added to byKey index and <seq, key> is added to bySeq Index. When a document is updated multiple times, they are pushed into the byKey Index as two update operations. We do not explicitly create a (delete old key, insert new key) and (delete old seq, insert new seq) operations. By not fetching previous document version during update, we save an extra IO operation required to fetch the old document. During the compaction operation, multiple versions for the same key are garbage collected.  Magma: A High Data Density Storage Engine Used in Couchbase Sarath Lakshman, Apaar Gupta, Rohan Suri, Scott Lashley, John Liang, Srinath Duvuru, Ravi Mayuram Couchbase, Inc {sarath,apaar.gupta,rohan.suri,scott.lashley,john.liang,srinath.duvuru,ravi}@couchbase.com ABSTRACT We present Magma, a write-optimized high data density key-value storage engine used in the Couchbase NoSQL distributed docu- ment database. Today’s write-heavy data-intensive applications like ad-serving, internet-of-things, messaging, and online gaming, generate massive amounts of data. As a result, the requirement for storing and retrieving large volumes of data has grown rapidly. Distributed databases that can scale out horizontally by adding more nodes can be used to serve the requirements of these internet- scale applications. To maintain a reasonable cost of ownership, we need to improve storage eciency in handling large data volumes per node, such that we don’t have to rely on adding more nodes. Our current generation storage engine, Couchstore is based on a log-structured append-only copy-on-write B+Tree architecture. To make substantial improvements to support higher data density and write throughput, we needed a storage engine architecture that lowers write amplication and avoids compaction operations that rewrite the whole database les periodically. We introduce Magma, a hybrid key-value storage engine that combines LSM Trees and a segmented log approach from log- structured le systems. We present a novel approach to perform- ing garbage collection of stale document versions avoiding index lookup during log segment compaction. This is the key to achieving storage eciency for Magma and eliminates the need for random I/Os during compaction. Magma oers signicantly lower write amplication, scalable incremental compaction, and lower space amplication while not regressing the read amplication. Through the eciency improvements, we improved the single machine data density supported by the Couchbase Server by 3.3x and lowered the memory requirement by 10x, thereby reducing the total cost of ownership up to 10x. Our evaluation results show that Magma outperforms Couchstore and RocksDB in write-heavy workloads. PVLDB Reference Format: Sarath Lakshman, Apaar Gupta, Rohan Suri, Scott Lashley, John Liang, Srinath Duvuru, and Ravi Mayuram. Magma: A High Data Density Storage Engine Used in Couchbase. PVLDB, 15(12): 3496-3508, 2022. doi:10.14778/3554821.3554839 1 INTRODUCTION Modern-day internet-scale interactive applications generate huge amounts of data through user engagements. These data-intensive This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. Proceedings of the VLDB Endowment, Vol. 15, No. 12 ISSN 2150-8097. doi:10.14778/3554821.3554839applications like ad-serving, internet-of-things, messaging, and on- line gaming are real-time and write-heavy, requiring large storage capacity and high transaction throughput. As a result, distributed databases that can scale horizontally have become an integral part of the modern data infrastructure stack that needs to operate at scale. The rapid growth of data volumes due to the digital wave has introduced challenges from a manageability and storage cost perspective. These problems have only grown despite the cost of computing and storage hardware like memory and ash dropping because the cost reduction has not kept up with the growth of data. The high throughput and storage capacity can be achieved by scaling out the distributed database by adding more nodes. To maintain a reasonable cost of ownership, we need to improve stor- age eciency in handling large data volumes per node, such that we don’t have to rely on adding more nodes. Under the hood, a single node of the distributed database depends on a persistent key-value storage engine for durable storage and retrieval of the database records. B+Trees [ 7] and Log structured merge trees [ 26] are two popular access methods for implementing persistent key-value storage engines. B+Tree is a read-optimized data structure while LSM Tree is write-optimized. Both of these data structures can be found in popular distributed databases like Couchbase, Cassandra, MongoDB, CockroachDB, etc. The eciency and performance of I/O intensive index structures are essentially a balance among three properties. Write amplication, read ampli- cation, and space amplication (RUM Conjecture) [ 4]. We cannot achieve write-optimized, read-optimized, and space-optimized per- sistent index structures all at the same time. Write amplication denes the ratio of the amount of data written to disk for every byte of write to the storage engine. Read amplication is the number of reads issued to the disk for every read operation of the storage engine. Space amplication is the ratio of the amount of data stored on a disk to the user input data size. Key Challenges with High Data Density. We start by identify- ing the challenges faced by our append-only copy-on-write B+Tree based storage engine to sustain high write throughput with a large volume of data per node with a database size to memory ratio of 100x. Slow Writes. Updates in a copy-on-write B+Tree are done as read-modify-write, requiring random read I/Os. As the density increases, reads incur large cache misses for the B+Tree pages. Keys are spread out in a large key range distribution, and hence larger B+Tree. The opportunity for deduplication before writing and amortization of page rewrites due to large batches reduces, thereby increasing the write amplication. Write latency increases and throughput drops. Compaction Challenges. When the database becomes frag- mented, a compaction operation needs to be performed to limit space amplication. Compaction performs a full database rewrite 3496 Magma: High Level Design Sarath Lakshman 2018-02-17 Introduction In this document we describe the components and the high level architecture of a storage engine which is suitable for meeting the KV-Engine storage requirements. The design thinking for this architecture is based on the evaluation of multiple storage engines . KV-Engine needs to maintain two internal storage data structures, by-id and by-seq . Each document has a sequence number and a key. KV-Engine issues point write operations (upsert or delete) to the database for each document mutation. The by-id index provides document lookup operation by document key. The by-seq index provides document iteration by a large range of sequence numbers. The by-seq range query should support scan stability with a snapshot of the database. The writes are atomic across by-id and by-seq. ie., if a crash happens while writing and the database recovers, the by-id and by-seq should not go out of sync. ie., writes across multiple indexes should be atomic. A local document store also needs to be provided by the storage engine for store metadata for the update. A log structured merge tree suits for a write-heavy, low latency point lookup workloads. There are additional advantages provided by an LSM architecture with a skewed workload distribution with small write working set of documents with temporal locality. However, when the documents become larger in size, an LSM oriented storage engine could suffer high write amplification. The segmented log based value store approach introduced by Plasma could significantly reduce write amplification when value sizes are bigger. A hybrid architecture approach would be a suitable fit for the KV-Engine storage management.   0 0.5 1 1.5 2 2.5 3 3.5 Update Round 1 Update Round 2Peak space amplificationMagma RocksDB Couchstore Magma Rollback and Crash Recovery LSM Tree Persistent Snapshots These additions to the LSM Tree API will allow for creation of snapshots of the tree that have been persisted to disk. The snapshots can be used for recovery in case of a crash, rollback to a snapshot during LSM Tree operation and handle restart of the Tree. Persistent Snapshot Creation A persistent snapshot is represented on disk via a state.x file where x is a version number of the snapshot represented by the file. The data written to the file includes current tree structure, walloffset, metadata of the latest batch and lsm tree sequence number. Creation of a new persistent snapshot will result in flush of all memtables. There are two types of persistent snapshots namely Rollback snapshots and Sync snapshots. They are essentially the same, only that the Sync snapshots rollover ie. creation of a new Sync or Rollback snapshot will result in removal of older Sync points whereas a queue of Rollback snapshots and maintained. Max size of the queue is configurable via the MaxRollbackPoints config variable. Status Sync() // Creates a Sync snapshot which is maintained only for as long as it is the latest persistent snapshot Status NewPersistentSnapshot(); // Create a rollback snapshot which is added to the rollback queue If upon creation, the new snapshot is found to have same Meta, WallOffset and LSM Tree Sequence number as the latest rollback snapshot, the rollback snapshot is discarded and the new snapshot is upgraded to a rollback snapshot. LSM Tree Rollback std::vector<RollbackPoint> GetAllRollbackPoints(); Provides a vector of persisted snapshots in order of creation.The vector contains MaxRollbackPoints number of rollback snapshots and a additional Sync snapshot. The Sync snapshot is only present if Sync() is called after the last Rollback Snapshot is created and LSM sequence number, meta or WALOffset has changed. The RollbackPoint object exposes GetMeta() and GetWALOffset(), allowing for selection of rollback points. Two rollback points will Delta Merge Operator and Transform Operator in Magma Merge operator can be used to implement lazy value materialization. This is similar to Plasma page deltas. Each document can stack up a few delta operations. When the document is read, all the delta operations as well as base document value is fetched. A configured custom merge function is used to generate an output document from deltas and base document. The merge function is called during compaction operation and replaces deltas+document with a new document Transform operator can be used to lazy transform documents or remove documents during the compaction operation. A custom transform operator function can be provided. Delta Merge Operator std::function<bool(const std::vector<Slice>& deltaValues, const Slice *baseValue, std::string& buffer, Slice*newDocVal)> baseValue can be nullptr if no old value is available Returns true if the newDocVal is a delta itself or false if a new document value. Delta(key, value) API An API similar to Put() is provided to add delta operations to a document. The merge function will receive deltas list in the same order in which they are pushed into the magma (FIFO). See the unit test for examples KV Engine document deletion The delta operator can be used to implement a logical delete operation. The KV Engine requires every document to be logically deleted and purged only when purge sequence number exceeds the document sequence number. The document value needs to have a deleted flag. When a delete is performed, we add a custom delete delta operation. The custom merge operator should identify the logical operator and create a new document value with the flag set. 